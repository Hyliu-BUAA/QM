{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced tour of the `baysian-optimization` package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Suggest-Evaluate-Register Paradigm\n",
    "1. Internally, the `bo.maximize()` method is simply a wrapper around the methods:\n",
    "    - `suggest()`\n",
    "    - `probe()`\n",
    "    - `register()`\n",
    "2. If you need `more control over your optimization loops` the `Suggest-Evaluate-Register` paradigm should give you that extra flexibility.\n",
    "\n",
    "<font color=\"red\">\n",
    "\n",
    "Note\n",
    "----\n",
    "1. For an example of running the `BayesianOptimization` in a `distributed fashion` (where the function being optimized is evaluated concurrently in different cores/machines/servers), checkout the `async_optimization.py` script in the examples folder.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by defining our function, bounds, and instanciating an optimization object.\n",
    "def black_box_function(x, y):\n",
    "    return -x ** 2 - (y-1) ** 2 + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Notice that the `evaluation of the black_box_function` will not be carried out by the `optimizer object`.\n",
    "4. We are simulating a situation where this function could be being executed in different machine, maybe it is written in another language, or it could even be the result of a chemistry experiment. \n",
    "5. Whatever the case may be, you can take charge of it and as long as you don't `invoke()` the `probe()` or `maximize()` methods directly, the optimizer object will ignore the blackbox function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "                f=black_box_function,\n",
    "                pbounds={'x':(-2, 2), 'y':(-3, 3)},\n",
    "                verbose=2,\n",
    "                random_state=1\n",
    "                )                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. One extra ingradient we will need is an `UtilityFunction` instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt import UtilityFunction\n",
    "\n",
    "utility = UtilityFunction(\n",
    "                kind='ucb',\n",
    "                kappa=2.5,\n",
    "                xi=0.0\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. The `suggest()` method of optimizer can be called at any time. \n",
    "    - What you get back is a `suggestion for the next parameter combination` the optimizer want to `probe`.\n",
    "\n",
    "<font color=\"red\">\n",
    "\n",
    "Note\n",
    "----\n",
    "1. Notice that `while the optimizer hasn't observed any points`, the suggestions will be `random`. However, they will `stop being random` and improve in quality the `more points are observed`.\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next point to probe is:  {'x': -0.331911981189704, 'y': 1.3219469606529488}\n"
     ]
    }
   ],
   "source": [
    "next_point_to_probe = optimizer.suggest(\n",
    "                        utility_function=utility\n",
    "                        )\n",
    "print(\"Next point to probe is: \", next_point_to_probe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. You are now free to evaluate your function at the `suggested point` however/whenever you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the target value to be: 0.7861845912690542\n"
     ]
    }
   ],
   "source": [
    "target = black_box_function(**next_point_to_probe)\n",
    "print(\"Found the target value to be:\", target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Last thing left to do is to `tell the optimizer what target value was observed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.register(\n",
    "                params=next_point_to_probe,\n",
    "                target=target\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. The maximize loop\n",
    "1. And that's it. By repeating the steps above you <font color=\"73DB90\">recreate the internal of the `maximize()` method.</font>\n",
    "    - This give you all the flexibility you need to log progress, hault execution, perform concurrent evalutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-18.49187152919165 {'x': 1.8861546000771092, 'y': -2.9917780942581977}\n",
      "0.7911494590443674 {'x': -0.31764604716962586, 'y': 1.3285597809731806}\n",
      "-7.0 {'x': -2.0, 'y': 3.0}\n",
      "-7.0 {'x': 2.0, 'y': 3.0}\n",
      "-7.50386768183302 {'x': -2.0, 'y': -1.122231769113124}\n",
      "{'target': 0.7911494590443674, 'params': {'x': -0.31764604716962586, 'y': 1.3285597809731806}}\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    next_point_to_probe = optimizer.suggest(\n",
    "                            utility_function=utility,\n",
    "                            )\n",
    "    target = black_box_function(**next_point_to_probe)\n",
    "    optimizer.register(\n",
    "                params=next_point_to_probe,\n",
    "                target=target\n",
    "                )\n",
    "\n",
    "    print(target, next_point_to_probe)\n",
    "\n",
    "    \n",
    "print(optimizer.max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dealing with discrete parameters\n",
    "1. There is no principed way of dealing with discrete parameters using this package.\n",
    "2. Ok, now that we got that out of the way, how do you do it? You're bound to be in a situation where some of your function's parameters may only take on discrete values. Unfortunately, the nature of bayesian optimization with gaussian processes doesn't allow for an easy/intuitive way of dealing with discrete parameters - but that doesn't mean it is impossible. The example below showcases a simple, yet reasonably adequate, way to dealing with discrete parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     w     |     x     |     y     |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m-0.06199 \u001b[0m | \u001b[0m 2.085   \u001b[0m | \u001b[0m 4.406   \u001b[0m | \u001b[0m-9.998   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m-0.0344  \u001b[0m | \u001b[95m 1.512   \u001b[0m | \u001b[95m-7.065   \u001b[0m | \u001b[95m-8.153   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m-0.2177  \u001b[0m | \u001b[0m 0.9313  \u001b[0m | \u001b[0m-3.089   \u001b[0m | \u001b[0m-2.065   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.1865  \u001b[0m | \u001b[95m 2.694   \u001b[0m | \u001b[95m-1.616   \u001b[0m | \u001b[95m 3.704   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-0.2187  \u001b[0m | \u001b[0m 1.022   \u001b[0m | \u001b[0m 7.562   \u001b[0m | \u001b[0m-9.452   \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.1868  \u001b[0m | \u001b[95m 2.533   \u001b[0m | \u001b[95m-1.728   \u001b[0m | \u001b[95m 3.815   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.05119 \u001b[0m | \u001b[0m 3.957   \u001b[0m | \u001b[0m-0.6151  \u001b[0m | \u001b[0m 6.785   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.1761  \u001b[0m | \u001b[0m 0.5799  \u001b[0m | \u001b[0m 1.181   \u001b[0m | \u001b[0m 4.054   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.04045 \u001b[0m | \u001b[0m 4.004   \u001b[0m | \u001b[0m 4.304   \u001b[0m | \u001b[0m 2.656   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.07509 \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 4.843   \u001b[0m | \u001b[0m 7.759   \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.3512  \u001b[0m | \u001b[95m 0.0     \u001b[0m | \u001b[95m-5.713   \u001b[0m | \u001b[95m 7.072   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m-0.8068  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m-9.09    \u001b[0m | \u001b[0m 8.6     \u001b[0m |\n",
      "| \u001b[95m 13      \u001b[0m | \u001b[95m 0.3774  \u001b[0m | \u001b[95m 0.3974  \u001b[0m | \u001b[95m-4.19    \u001b[0m | \u001b[95m 6.264   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.157   \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m-3.587   \u001b[0m | \u001b[0m 8.534   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m-0.7891  \u001b[0m | \u001b[0m 0.4794  \u001b[0m | \u001b[0m-5.536   \u001b[0m | \u001b[0m 4.298   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.1176  \u001b[0m | \u001b[0m 1.038   \u001b[0m | \u001b[0m-4.671   \u001b[0m | \u001b[0m 7.41    \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.2073  \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m-3.028   \u001b[0m | \u001b[0m 6.698   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.1998  \u001b[0m | \u001b[0m 1.662   \u001b[0m | \u001b[0m-2.996   \u001b[0m | \u001b[0m 5.998   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.08394 \u001b[0m | \u001b[0m 1.398   \u001b[0m | \u001b[0m-0.5096  \u001b[0m | \u001b[0m 5.287   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.1608  \u001b[0m | \u001b[0m 1.282   \u001b[0m | \u001b[0m-0.3192  \u001b[0m | \u001b[0m 2.604   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.08846 \u001b[0m | \u001b[0m 2.849   \u001b[0m | \u001b[0m 1.07    \u001b[0m | \u001b[0m 3.579   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.1882  \u001b[0m | \u001b[0m 0.493   \u001b[0m | \u001b[0m 1.955   \u001b[0m | \u001b[0m 1.911   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.1358  \u001b[0m | \u001b[0m 0.3973  \u001b[0m | \u001b[0m 3.516   \u001b[0m | \u001b[0m 3.594   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.2241  \u001b[0m | \u001b[0m 2.341   \u001b[0m | \u001b[0m 1.17    \u001b[0m | \u001b[0m 0.6908  \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.1587  \u001b[0m | \u001b[0m 1.693   \u001b[0m | \u001b[0m 3.177   \u001b[0m | \u001b[0m 0.2292  \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 0.4179  \u001b[0m | \u001b[0m 1.037   \u001b[0m | \u001b[0m-0.3008  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.0     \u001b[0m | \u001b[0m 3.662   \u001b[0m | \u001b[0m-0.4826  \u001b[0m | \u001b[0m 1.228   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.2246  \u001b[0m | \u001b[0m 3.593   \u001b[0m | \u001b[0m 2.518   \u001b[0m | \u001b[0m-0.6599  \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.09242 \u001b[0m | \u001b[0m 3.394   \u001b[0m | \u001b[0m 4.537   \u001b[0m | \u001b[0m-1.403   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.08199 \u001b[0m | \u001b[0m 4.324   \u001b[0m | \u001b[0m 2.351   \u001b[0m | \u001b[0m 0.9946  \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "def func_with_discrete_params(x, y, d):\n",
    "    # Simulate necessity of having d being discrete.\n",
    "    assert type(d) == int\n",
    "    \n",
    "    return ((x + y + d) // (1 + d)) / (1 + (x + y) ** 2)\n",
    "\n",
    "def function_to_be_optimized(x, y, w):\n",
    "    d = int(w)\n",
    "    return func_with_discrete_params(x, y, d)\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=function_to_be_optimized,\n",
    "    pbounds={'x': (-10, 10), 'y': (-10, 10), 'w': (0, 5)},\n",
    "    verbose=2,\n",
    "    random_state=1,\n",
    ")\n",
    "\n",
    "optimizer.maximize(alpha=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tuning the underlying `Gaussian Process`\n",
    "1. The bayesian optimization algorithm works by `performing a gaussian process regression (代理模型)` of the observed combination of parameters and their associated target values. The `predicted parameter` $\\rightarrow$ `target hyper-surface (and its uncertainty)` is then used to `guide the next best point to probe`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Passing parameter to the GP\n",
    "1. Depending on the problem it could be beneficial to `change the default parameters of the underlying Gaussian Process`. You can simply pass `GP parameters` to the `maximize()` method directly as you can see below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     x     |     y     |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.7862  \u001b[0m | \u001b[0m-0.3319  \u001b[0m | \u001b[0m 1.322   \u001b[0m |\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m-18.49   \u001b[0m | \u001b[0m 1.886   \u001b[0m | \u001b[0m-2.992   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.7911  \u001b[0m | \u001b[95m-0.3176  \u001b[0m | \u001b[95m 1.329   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m-6.11    \u001b[0m | \u001b[0m-1.763   \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m-2.895   \u001b[0m | \u001b[0m 1.533   \u001b[0m | \u001b[0m 2.243   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m-4.806   \u001b[0m | \u001b[0m-2.0     \u001b[0m | \u001b[0m-0.3439  \u001b[0m |\n",
      "=================================================\n"
     ]
    }
   ],
   "source": [
    "optimizer = BayesianOptimization(\n",
    "                    f=black_box_function,\n",
    "                    pbounds={'x':(-2, 2), 'y':(-3, 3)},\n",
    "                    verbose=2,\n",
    "                    random_state=1,\n",
    "                    )\n",
    "\n",
    "optimizer.maximize(\n",
    "            init_points=1,\n",
    "            n_iter=5,\n",
    "            # What follows are GP regressor parameters\n",
    "            alpha=1e-3,\n",
    "            n_restarts_optimizer=5,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Another alternative, specially useful if you're calling `maximize() multiple times` or `optimizing outside the maximize loop`, is to call the `set_gp_params()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.set_gp_params(normalize_y=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Tuning the `alpha` parameter\n",
    "1. When dealing with functions with `discrete parameters`,or particularly `erratic target space` it might be beneficial to `increase the value of the alpha parameter`. \n",
    "2. This parameters controls `how much noise the GP can handle`, so increase it whenever you think that extra flexibility is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Changing `kernel function`\n",
    "<font color=\"73DB90\" size=\"4\">\n",
    "\n",
    "You can change kernel function\n",
    "------------------------------\n",
    "1. By default this package uses the `Mattern 2.5 kernel`. Depending on your use case you may find that tunning the `GP kernel` could be beneficial. You're on your own here since these are very specific solutions to very specific problems.\n",
    "2. 如果想要更改`kernel function`，则需要自己改代码加功能\n",
    "\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Observers Continued\n",
    "1. Observers are objects that `subscribe and listen to particular events` by the `BayesianOptimization` object.\n",
    "2. When an event gets fired a callback function is called with the event and the BayesianOptimization instance passed as parameters. The callback can be specified at the time of subscription. If none is given it will look for an update method from the observer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bayes_opt.event import DEFAULT_EVENTS, Events\n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "                f=black_box_function,\n",
    "                pbounds={'x': (-2, 2), 'y': (-3, 3)},\n",
    "                verbose=2,\n",
    "                random_state=1,\n",
    ")\n",
    "\n",
    "\n",
    "class BasicObserver:\n",
    "    def update(self, event, instance):\n",
    "        '''\n",
    "        Description\n",
    "        -----------\n",
    "            1. Does whatever you want with the event and `BayesianOptimization` instance.\n",
    "        '''\n",
    "        print(\"Event `{}` was observed\".format(event))\n",
    "\n",
    "\n",
    "my_observer = BasicObserver()\n",
    "optimizer.subscribe(\n",
    "    event=Events.OPTIMIZATION_STEP,\n",
    "    subscriber=my_observer,\n",
    "    callback=None, # Will use the `update` method as callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Alternatively you have the option to pass a completely different callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go nuts here!\n",
      "Event `optimization:step` was observed\n",
      "Event `optimization:step` was observed\n",
      "Event `optimization:step` was observed\n"
     ]
    }
   ],
   "source": [
    "def my_callback(event, instance):\n",
    "    print(\"Go nuts here!\")\n",
    "\n",
    "optimizer.subscribe(\n",
    "    event=Events.OPTIMIZATION_START,\n",
    "    subscriber=\"Any hashable object\",\n",
    "    callback=my_callback,\n",
    ")\n",
    "\n",
    "optimizer.maximize(init_points=1, n_iter=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. For a list of all default events you can checkout `DEFAULT_EVENTS`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['optimization:start', 'optimization:step', 'optimization:end']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEFAULT_EVENTS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('research')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7b361d90544c21c7e570702d0c4d23653c8dcac4c1ecf309667aae54eeacb0d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
