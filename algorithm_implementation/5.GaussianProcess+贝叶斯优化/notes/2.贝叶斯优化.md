# 0. 贝叶斯优化 (`Bayesian Optimization`)
<font color="steelblue" size="4">

Website
-------
1. https://zhuanlan.zhihu.com/p/150555551
2. https://zhuanlan.zhihu.com/p/390373572

Introduction
------------
1. `Bayesian optimization` 可以在某个区间最大化某个函数
$$max_{x \in \mathcal{X}} f(x)$$
2. 如果这个函数计算比较容易，甚至还可以知道它的梯度，那事情就好办了，一阶、二阶优化算法换着上就完事。
3. 但现实往往没有那么理想，这个函数的一阶、二阶导数信息我们可能是没有的，甚至`计算一次函数的值都很费劲`
   1. （给定一个 $x$，计算 $f(x)$ 的计算量很大，比如神经网络中的超参数优化）
   2. 这时候就要求助 `gradient-free 的优化算法`了，这类算法也很多了，`贝叶斯优化`就属于`无梯度优化算法`中的一种，<font color="red">它希望在尽可能少的试验情况下去尽可能获得优化命题的`全局最优解`。</font>

</font>

# 1. `Bayesian Optimization` 概述
## 1.1. `代理模型` && `Acquisition Function`
<font color="steelblue" size="4">

1. 由于我们要优化的这个`函数计算量太大`(比如`DFT-NEB计算势垒`)，我们需要使用一个`代理模型`来近似 $f(x)$。
2. `Bayesian Optimization` 中使用的代理模型是`高斯过程 (Gaussian Process)`。
3. 假设我们对待优化函数 $f(x)$ 的`先验 (prior)`为高斯过程。经过一定的试验我们有了`数据 (evidence)`。然后根据 `贝叶斯定理 (Bayesian pricipal)` 就可以得到这个函数的`后验分布`。
4. 有了这个`后验分布`，我们`需要考虑的是下一次试验点在哪里进一步收集数据`。因此需要构造一个 `acquisition function` 用于指导搜索方向（选择下一个试验点）。
5. 然后去试验，得到数据后更新 `代理模型` 的后验分布，反复进行。

</font>

## 1.2. Bayesian Optimization 过程
<font color="steelblue" size="4">

1. for t = 1, 2, ... do
2. Find $x_t$ by optimizing the `acquisition function` over the `Gaussian Process`: $x_t = arg\max_x u(x|D_{1:t-1})$
3. Sample the `objective function`: $y_t = f(x_t) + \epsilon_t$
4. Augment the data $D_{1:t} = \{ D_{1:t-1}, (x_t; y_t) \}$ and `update the GP`
5. end for

</font>


# 2. Gaussian Process (高斯过程)
<font color="steelblue" size="4">

1. 高斯过程是多元高斯分布向无穷维的扩展。
    - `高斯分布`是`随机变量的分布`
    - `高斯过程`是`函数的分布`，它由`均值向量`和`协方差矩阵(核函数)`确定
$$f(\vec{x}) \sim \mathcal{GP}(\vec{\mu}, k(\vec{x}, \vec{x^{'}}))$$
2. 在 t 次试验后，我们有数据 $\{ x_{1:t}, f_{1:t} \}$，由于高斯过程上任意点 $f_{t+1}$ 与之前的观测数据`服从联合高斯分布`。进一步可以得到预测分布：
$$P(f_{t+1}| D_{1:t, x_{t+1}}) \sim \mathcal{N}(\mu_t(x_{t+1}), \sigma_t^2(x_{t+1}))$$
3. 我们可以`根据高斯过程的后验分布`对这个未知函数在任意位置的值做出`预测均值、方差`。

</font>


# 3. `Acquisition Function` -- 指明搜索方向
<font color="steelblue" size="4">

1. 为了`搜到目标函数的最优解`，`贝叶斯优化`选择的搜索方向为：
    - `预测值大`的位置
    - `不确定性大`的位置
2. 因此 `Bayesian Optimization` 中很多工作关注点在于 `acquisition function` 的设计
$$ arg\max_x{u(x|D)} $$
3. `代理模型`的`均值和不确定性的线性组合`就是一种能简单的权衡`勘探/开发(exploration/exploitation)`的采集函数
    - `均值`表示`开发`（我们模型的知识
    - `不确定性`则表示`探索`（由于我们的模型缺乏观测）。

</font>

## 3.1. 最大化提升`概率` -- `PI`
<font color="steelblue" size="4">

1. 最容易想到的就是我希望下一次试验的结果比当前所有观测结果都要好 ($x^+ = arg\max_{x_i \in x_{1:t}}f(x)$)。或者说，`新采样的函数值更优的概率要大`
$\begin{aligned}
PI(x) &= P(f(x) \ge f(x^+)) \\
      &= \Phi \left(\frac{\mu(x)-f(x^+)}{\sigma(x)} \right)
\end{aligned}$
2. 但是仅仅这样考虑是有点目光短浅的，它`忽略了对不确定性的考虑`，一味追求选择大`概率`肯定大于 $f(x^+)$ 的点，也就是一直在`exploitation`
    - 这样的缺点是`可能就陷入了局部最优`，`忽略了潜在的最优解`。
    - 改进的方法也很简单，`加个偏置`就可以了
$\begin{aligned}
PI(x) &= P(f(x) \ge f(x^+) + \xi) \\
      &= \Phi \left(\frac{\mu(x)-f(x^+)-\xi}{\sigma(x)} \right)
\end{aligned}$
    - $\xi$ 为`超参数`，可以指导模型去更多地探索 (exploration)，从而获得更优的解

</font>

## 3.2. 最大化提升`量` -- `EI`
<font color="steelblue" size="4">

1. 提升的概率大并不意味着提升得多
2. 另一种量化的角度就是`考虑提升量`（可以不严谨地理解为梯度下降法中，不仅要下降，而且要下得更多一点）
$\begin{aligned}
I(x) = \max{(0, f_{t+1}(x) - f(x^+))}
\end{aligned}$
3. 那么要求得下一次试验点就可以`最大化期望的提升量`
$\begin{aligned}
x = arg\max_x{E[I(x)]}
\end{aligned}$
4. 由于`代理模型`为`高斯过程`，提升量 $I$ 的似然满足标准正态分布，因此得到:
$f(x) = \begin{cases}
\left[ (\mu(x) - f(x^+))\Phi(Z) + \sigma(x)\phi(Z) \right], \space \sigma(x)>0  \\
0, \space \sigma(x) \le 0
\end{cases}$
   - $Z = \frac{\mu(x)-f(x^+)}{\sigma(x)}$
5. 同样也可以引入超参数 $\xi$ 用于控制 `exploration` 和 `exploitation`
$f(x) = \begin{cases}
\left[ (\mu(x) - f(x^+) -\xi)\Phi(Z) + \sigma(x)\phi(Z) \right], \space \sigma(x)>0  \\
0, \space \sigma(x) \le 0
\end{cases}$

</font>

## 3.3. 最大化`置信上界` -- `UCB`
<font color="steelblue" size="4">

1. 

</font>