# Gradient Descent (GD, 梯度下降法)

# 1. The content of `gradient descent algorithm`
## 1.1. Prerequisite
1. `Target function (目标函数)`: $ h_\theta(x) $ 
2. `Cost function (损失函数)`  : $ J(\theta) $

## 1.2. The role of `GD algorithm`
1. `GD algorithm`: find the optimal value of $\theta$ such that the `cost function` $ J(\theta) $ is minimum. 

## 1.3. Formular
$$\begin{aligned}
&Repeat \quad until \quad convergence \\
&\{  \\
&\theta_j := \theta_j - m \frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1, ...)  \\ 
&\} 
\end{aligned}$$

- $\theta$: The parameter to be optimized
- $J(\theta)$: The cost function
- $\alpha$: Learning rate
- $m$: The number of training examples

## 1.4. Cost function $J(\theta)$ 求导
1. 最常见的平方误差`损失函数(cost function)`：
$$\begin{aligned}
&J(\theta_0, \theta_1, ...) = \frac{1}{2m}\sum_{i=1}^{m}{\left[ h_{\theta}(x^{(i)}) - y^{(i)} \right]^2}
\end{aligned}$$
2. 则损失函数$J(\theta)$求导结果为：
$$\begin{aligned}
\frac{\partial}{\partial \theta_j}J(\theta_j) = \frac{1}{m}\sum_{i=1}^{m}{\left[ h_{\theta}(x^{(i)}) - y^{(i)} \right] \frac{\partial}{\partial \theta_j}h_{\theta}(x^{(i)}_j)}
\end{aligned}$$


## 1.5. <font color="red">Note</font>
### 1.5.1. You should update $\theta_1, \theta_2$ `simultaneously`

$$\begin{aligned}
&tmp0 := \theta_0 - m\frac{\partial}{\partial \theta_0}J(\theta_0, \theta_1) \\
&tmp1 := \theta_1 - m\frac{\partial }{\partial \theta_1}J(\theta_0, \theta_1)    \\
&\theta_0 := tmp0 \\
&\theta_1 := tmp1
\end{aligned}$$

### 1.5.2. <font color="red">Incorrect: 错误的示范 -- 没有同时更新 $\theta_0, \theta_1$</font>

$$\begin{aligned}
&tmp0 := \theta_0 - m\frac{\partial}{\partial \theta_0}J(\theta_0, \theta_1) \\
&\theta_0 := tmp0 \\
&tmp1 := \theta_1 - m\frac{\partial }{\partial \theta_1}J(\theta_0, \theta_1)    \\
&\theta_1 := tmp1
\end{aligned}$$


# 2. `Gradient descent algorithm` && `Linear Regression` (hypothesis function only includes $\theta_0, \theta_1$)

## 2.1. `Gradient descent`

$$\begin{aligned}
&Repeat \quad until \quad convergence \\
&\{ \\
&\theta_j := \theta_j - m\frac{\partial}{\partial \theta_j}J(\theta_0, \theta_1)   \\
&(for \quad j=0 \quad and \quad j=1) \\
&\}
\end{aligned}$$


## 2.2. `Linear Regression Model`
$$\begin{aligned}
&h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 \\
&J(\theta_0, \theta_1) = \frac{1}{2m}\sum_{i=1}^m{( h_{\theta}(x^{(i)}) - y^{(i)} )^2}
\end{aligned}$$

### 2.2.1 损失函数$J(\theta)$求导结果：
1. For $\theta_0$:
$$\begin{aligned}
\frac{\partial}{\partial \theta_0}J(\theta_0) = \frac{1}{m}\sum_{i=1}^{m}{\left[ \theta_0 + \theta_1 x^{(i)}_1 + \theta_2 x^{(i)}_2 - y^{(i)} \right]}
\end{aligned}$$
2. For $\theta_1$ and $\theta_2$: 
$$\begin{aligned}
\frac{\partial}{\partial \theta_j}J(\theta_j) = \frac{1}{m}\sum_{i=1}^{m}{\left[ \theta_0 + \theta_1 x^{(i)}_1 + \theta_2 x^{(i)}_2 - y^{(i)} \right] x^{(i)}_j}
\end{aligned}$$


# 3. Vectorized Implementation of `Gradient Descent` in `Linear Regression`

## 2.1. Prerequisite
### 2.1.1. Matrix and vector
1. Training Set's `Features`:
$$\begin{aligned}
X = \begin{bmatrix} 1 & x^1_1 & x^1_2 & x^1_3 \\ 1 & x^2_1 & x^2_2 & x^2_3 \\ 1 & x^3_1 & x^3_2 & x^3_3 \\ 1 & x^4_1 & x^4_2 & x^4_3 \\ 1 & x^5_1 & x^5_2 & x^5_3 \end{bmatrix}
\end{aligned}$$

- $ X $ 是一个 `m * n` 矩阵，`m` 是训练集样本数，`n` 是特征数
- 本训练集包含 5 个样本，有 4 个features

2. Vector of hyperparamters
$$\begin{aligned}
\Theta = \begin{bmatrix} \theta_1 \\ \theta_2 \\ \theta_3 \\ \theta_4  \end{bmatrix}
\end{aligned}$$

3. Training Set's `Target`:
$$\begin{aligned}
Y = \begin{bmatrix} y^1 \\ y^2 \\ y^3 \\ y^4 \end{bmatrix}
\end{aligned}$$

## 2.2. Vectorized formula
$$\begin{aligned}
&Repeat \quad until \quad convergence \\
&\{ \\
&\Theta = \Theta - (\Theta X^T - Y) \\
&\} \\
\end{aligned}$$

<font color="red" size="2">

Note
----
1. 以上形式减少了 `for` 循环次数，把 `样本个数` 次循环减少为 `1` 次向量计算

</font>

## 2.3. Proof from 2.2.
$$\begin{aligned}
\Theta X^T 
&=\begin{bmatrix} \theta_0 \\ \theta_1 \\ \theta_2 \\ \theta_3 \end{bmatrix} \begin{bmatrix} 1 & x^1_1 & x^1_2 & x^1_3 \\ 1 & x^2_1 & x^2_2 & x^2_3 \\ 1 & x^3_1 & x^3_2 & x^3_3 \\ 1 & x^4_1 & x^4_2 & x^4_3 \\ 1 & x^5_1 & x^5_2 & x^5_3 \end{bmatrix} ^T \\ 
&=\begin{bmatrix} \theta_0 \\ \theta_1 \\ \theta_2 \\ \theta_3 \end{bmatrix} \begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\ x^1_0 & x^2_0 & x^3_0 & x^4_0 & x^5_0 \\ x^1_1 & x^2_1 & x^3_1 & x^4_1 & x^5_1  \\ x^1_2 & x^2_2 & x^3_2 & x^4_2 & x^5_2 \\ x^1_3 & x^2_3 & x^3_3 & x^4_3 & x^5_3 \end{bmatrix} \\ 
&= \begin{bmatrix} h_{\theta}(x^{(1)}) \\ h_{\theta}(x^{(2)}) \\ h_{\theta}(x^{(3)}) \\ h_{\theta}(x^{(4)}) \\ h_{\theta}(x^{(5)}) \end{bmatrix}
\end{aligned}$$

$$\begin{aligned}
\Theta X^T - Y 
&= \begin{bmatrix} h_{\theta}(x^{(1)}) - y^{(1)} \\ h_{\theta}(x^{(2)}) - y^{(2)} \\ h_{\theta}(x^{(3)}) - y^{(3)} \\ h_{\theta}(x^{(4)}) - y^{(4)} \\ h_{\theta}(x^{(5)}) - y^{(5)} \end{bmatrix}  \\
&= \begin{bmatrix} E^1 \\ E^2 \\ E^3 \\ E^4 \\ E^5 \end{bmatrix}
\end{aligned}$$

$$\begin{aligned}
(\Theta X^T - Y)X 
&= \begin{bmatrix} E^1 \\ E^2 \\ E^3 \\ E^4 \\ E^5 \end{bmatrix} \begin{bmatrix} 1 & x^1_1 & x^1_2 & x^1_3 \\ 1 & x^2_1 & x^2_2 & x^2_3 \\ 1 & x^3_1 & x^3_2 & x^3_3 \\ 1 & x^4_1 & x^4_2 & x^4_3 \\ 1 & x^5_1 & x^5_2 & x^5_3 \end{bmatrix}    \\
&= \begin{bmatrix} E^1 + E^2 + E^3 + E^4 + E^5 \\ E^1 x^1_1 + E^2 x^2_1 + E^3 x^3_1 + E^4 x^4_1 + E^5 x^5_1 \\ E^1 x^1_2 + E^2 x^2_2 + E^3 x^3_2 + E^4 x^4_2 + E^5 x^5_2 \\ E^1 x^1_3 + E^2 x^2_3 + E^3 x^3_3 + E^4 x^4_3 + E^5 x^5_3  \end{bmatrix}
\end{aligned}$$