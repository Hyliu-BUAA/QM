# 学习网址
https://www.jb51.net/article/146609.htm

# 1. 批量梯度下降法
1. 每次使用全量的训练集样本来更新模型参数，即给定一个步长，然后对所有的样本的梯度的和进行迭代
2. 梯度下降算法最终得到的是局部极小值。而线性回归的损失函数为`凸函数`，有且只有一个局部最小，则这个局部最小一定是全局最小。所以线性回归中使用批量梯度下降算法，一定可以找到一个全局最优解。


# 2. 随机梯度下降法
1. 随机梯度下降算法每次只`随机选择一个样本来更新模型参数`，因此每次的学习是非常快速的，并且可以进行在线更新。 
2. 随机梯度下降最大的`缺点`在于每次更新可能并不会按照正确的方向进行，因此可以`带来优化波动(扰动)`。不过从另一个方面来看，随机梯度下降所带来的波动有个好处就是，对于类似盆地区域（即很多局部极小值点）那么这个波动的特点可能会使得优化的方向`从当前的局部极小值点跳到另一个更好的局部极小值点`，这样便可能`对于非凸函数`，最终收敛于一个较好的局部极值点，甚至全局极值点。 


# 3. Mini-batch 梯度下降
1. `Mini-batch梯度下降`综合了batch梯度下降与stochastic梯度下降，在每次更新速度与更新次数中间取得一个平衡，其每次更新`从训练集中随机选择b,b<m个样本进行学习`