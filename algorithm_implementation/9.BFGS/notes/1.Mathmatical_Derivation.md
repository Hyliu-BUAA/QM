<font color="#73BD90" size="4">

Website
-------
1. https://towardsdatascience.com/bfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504
    1. `Gradient descent` reviewed
    2. `Newton's Method`
    3. `Quasi-Newton Method`
    4. `BFGS optimization`

</font>


# 1. Newton Method
<font color="steelblue" size="4">

1. The `aim` of `Newton Method`:
$$\begin{aligned}
\min_{\vec{x}\in\mathbb{R^n}}{f(\vec{x})}
\end{aligned}$$
2. 将 $f(x)$ 做二阶展开
$$\begin{cases}
f(x+\mathbb{v}) = f(x) + \nabla f(x)^T\mathbb{v} \\
\mathbb{v} = \Delta x
\end{cases}$$
3. 我们需要 $f(x+\mathbb{v})$ 最小，我们把 $x$ 看做一个常数，使用 $f(x+\mathbb{v})$ 对 $\mathbb{v}$ 求导：
$$\begin{aligned}
\frac{\partial}{\partial \mathbb{v}}f(x+\mathbb{v}) = \nabla f(x) + \nabla^2f(x)\mathbb{v}=0
\end{aligned}$$
4. <font color="red">`Search direction` of `Newton Method`:</font>
$$\begin{aligned}
\mathbb{v} = -\nabla^2f(x)^{-1}\nabla f(x)
\end{aligned}$$
5. `Hessian Matrix`:
$$\begin{aligned}
\nabla^2f(x)
\end{aligned}$$
6. `Hessian Matrix` 的`正定性`：
$$\begin{aligned}
\nabla{f(x)^T} \mathbb{v} = -\nabla f(x)^T \nabla^2 f(x)^{-1}\nabla{f(x)} \ge 0
\end{aligned}$$

</font>


# 2. Quasi-Newton Method
<font color="steelblue" size="4">

1. An `optimization method` that `leverages the approximate second-order behavior of the object function` in order to converge faster `without actually taking any derivetives`.
2. 使用一个 $B_{k+1}$ 近似 Hessian 矩阵
    - 保证 $B_{k+1}$ 的`正定 (positive-definiteness)`：$$\Delta x_{k}^T B_{k+1}\Delta x_{k} = \Delta x_k^T y_k \ge 0$$
3. 公式：
$$\begin{cases}
B_{k+1}\Delta x_k = y_k \\
y_k = \nabla f(x_{k+1}) - \nabla f(x_k) \\
\Delta x_k = x_{k+1} - x_k  \\
\end{cases}$$

</font>

<font color="coral" size="4">

Quasi-Newton Method
-------------------
$$\begin{aligned}
B_{k+1} \Delta x_k = y_k
\end{aligned}$$

</font>

## 2.1. `One-dimensional` Quasi-Newton Method
<font color="steelblue" size="4">

1. One-dimensional:
$$\begin{aligned}
f''(x_{k+1})(x_{k+1} - x_k) = f'(x_{k+1}) - f'(x_k)
\end{aligned}$$
2. `Curvature condition` is satisfied by requiring
$$\begin{aligned}
\frac{f'(x_{k+1}-f'(x_k))}{x_{k+1}-x_k}
\end{aligned}$$
3. 保证 `Hessian Matrix` 的`正定性 (positive-definiteness)`：
$$\begin{aligned}
\frac{f'(x_{k+1}-f'(x_k))}{x_{k+1}-x_k} > 0
\end{aligned}$$
4. `Update scheme` in Newton Method:
    - 用到了 $x_{k-1}$, $x_k$, $x_{k+1}$ 三个点
$$\begin{aligned}
x_{k+1} = x_k - f'(x_k)\frac{x_k - x_{k-1}}{f'(x_k) - f'(x_{k-1})}
\end{aligned}$$

</font>

## 2.2. `Multiple-dimensional` 
<font color="steelblue" size="4">

1. For `multiple-dimensions` problem, there are $\frac{n(n+1)}{2}$ 项偏导数需要求

</font>