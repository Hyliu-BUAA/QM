<font color="#73BD90" size="4">

Website
-------
1. https://towardsdatascience.com/bfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504
    1. `Gradient descent` reviewed
    2. `Newton's Method`
    3. `Quasi-Newton Method`
    4. `BFGS optimization`

</font>


# 1. Newton Method
<font color="steelblue" size="4">

1. The `aim` of `Newton Method`:
$$\begin{aligned}
\min_{\vec{x}\in\mathbb{R^n}}{f(\vec{x})}
\end{aligned}$$
2. 将 $f(x)$ 做二阶展开
$$\begin{cases}
f(x+\mathbb{v}) = f(x) + \nabla f(x)^T\mathbb{v} \\
\mathbb{v} = \Delta x
\end{cases}$$
3. 我们需要 $f(x+\mathbb{v})$ 最小，我们把 $x$ 看做一个常数，使用 $f(x+\mathbb{v})$ 对 $\mathbb{v}$ 求导：
$$\begin{aligned}
\frac{\partial}{\partial \mathbb{v}}f(x+\mathbb{v}) = \nabla f(x) + \nabla^2f(x)\mathbb{v}=0
\end{aligned}$$
4. <font color="red">`Search direction` of `Newton Method`:</font>
$$\begin{aligned}
\mathbb{v} = -\nabla^2f(x)^{-1}\nabla f(x)
\end{aligned}$$
5. `Hessian Matrix`:
$$\begin{aligned}
\nabla^2f(x)
\end{aligned}$$
6. `Hessian Matrix` 的`正定性`：
$$\begin{aligned}
\nabla{f(x)^T} \mathbb{v} = -\nabla f(x)^T \nabla^2 f(x)^{-1}\nabla{f(x)} \ge 0
\end{aligned}$$

</font>


# 2. Quasi-Newton Method
<font color="steelblue" size="4">

1. An `optimization method` that `leverages the approximate second-order behavior of the object function` in order to converge faster `without actually taking any derivetives`.
2. 使用一个 $B_{k+1}$ 近似 Hessian 矩阵
    - 保证 $B_{k+1}$ 的`正定 (positive-definiteness)`：$$\Delta x_{k}^T B_{k+1}\Delta x_{k} = \Delta x_k^T y_k \ge 0$$
3. 公式：
$$\begin{cases}
B_{k+1}\Delta x_k = y_k \\
y_k = \nabla f(x_{k+1}) - \nabla f(x_k) \\
\Delta x_k = x_{k+1} - x_k  \\
\end{cases}$$

</font>

<font color="coral" size="4">

Quasi-Newton Method
-------------------
$$\begin{aligned}
B_{k+1} \Delta x_k = y_k
\end{aligned}$$

</font>

## 2.1. `One-dimensional` Quasi-Newton Method
<font color="steelblue" size="4">

1. One-dimensional:
$$\begin{aligned}
f''(x_{k+1})(x_{k+1} - x_k) = f'(x_{k+1}) - f'(x_k)
\end{aligned}$$
2. `Curvature condition` is satisfied by requiring
$$\begin{aligned}
\frac{f'(x_{k+1}-f'(x_k))}{x_{k+1}-x_k}
\end{aligned}$$
3. 保证 `Hessian Matrix` 的`正定性 (positive-definiteness)`：
$$\begin{aligned}
\frac{f'(x_{k+1}-f'(x_k))}{x_{k+1}-x_k} > 0
\end{aligned}$$
4. `Update scheme` in Newton Method:
    - 用到了 $x_{k-1}$, $x_k$, $x_{k+1}$ 三个点
$$\begin{aligned}
x_{k+1} = x_k - f'(x_k)\frac{x_k - x_{k-1}}{f'(x_k) - f'(x_{k-1})}
\end{aligned}$$

</font>

## 2.2. `Multiple-dimensional` 
<font color="steelblue" size="4">

1. For `multiple-dimensions` problem, there are $\frac{n(n+1)}{2}$ 项偏导数需要求

</font>


# 3. BFGS
## 3.1. Additional Constraints of `BFGS`
<font color="steelblue" size="4">

1. We saw previously that in $n>1$ dimensions, `quasi-Newton condition` is `underdetermined`
2. To determine an `update scheme` for $B$, we need to impose `additional constraints`.
3. <font color="red">One such constraints that we've already mention is the `symmetry` and `positive-definiteness` of $B$.</font>
    - These properties should be preserved in each update.
4. <font color="red">Another desirable property we want is for $B_{k+1}$ we want is for $B_{k+1}$ to be `sufficiently close to` $B_k$</font>
    - A formal way to characterize the notion of `closeness` for matrices is the `matrix norm`.
5. 总结 `3` 和 `4` 成如下公式:
$$\begin{cases}
\min_{k+1}{||B_{k+1}-B_k||} \\
B_{k+1}^T = B_{k+1} \\
B_{k+1}\Delta x_k = y_k = \nabla{f(x_{k+1})} - \nabla{f(x_k)}
\end{cases}$$
6. Update the `approximate inverse of Hessian Matrix` $B_{k+1}$:
$$\begin{cases}
\min_{k+1}{||B^{-1}_{k+1}-B^{-1}_k||} \\
B^{-1\quad T}_{k+1} = B^{-1}_{k+1} \\
\Delta x_k = B^{-1}_{k+1}y_k = B^{-1}_{k+1}(\nabla{f(x_{k+1})} - \nabla{f(x_k)})
\end{cases}$$
7. We have also previously mentioned many time the requirement that $B$ (and by extension $B^{-1}$) be `positive-definite`; it turns out this property comes along for the ride when we solve this problem.
8. To solve for $B_{k+1}$, we still have to specify the particular `matrix norm`. `Difference choices of norms` result in `different quasi-Newton methods`, characterized by a different resulting `update scheme`.
    - In the BFGS method, the norm is chosen to be the `Frobenius norm`:
$$\begin{aligned}
||A||_F = \sqrt{\sum^m_i\sum^n_j |a_{ij}|^2}
\end{aligned}$$
    - which is just the square root of the sum of the absolute value squared of the matrix element.

</font>


## 3.2. Deritives of `BFGS` -- Update `approximation of Hessian Matrix` $B_k$
<font color="steelblue" size="4">

1. For our purpose, it suffices to know that the problem end up being equivalent to `updating our approximate Hessian` at each iteration by `adding two symmetric, rank-one matrices` $U$ and $V$:
$$\begin{aligned}
B_{k+1} = B_{k} + U_{k} + V_{k}
\end{aligned}$$
2. The update matrices can then be chosen to be of the form:
    - $\vec{u}$, $\vec{v}$: `linearly independent non-zero vectors`
    - $a, b$: `Constants`
$$\begin{cases}
U = a \vec{u} \vec{u}^T \\
V = b \vec{v} \vec{v}^T
\end{cases}$$
    - The `outer product` of any two non-zero vectors ($\vec{u}\vec{u}^T$) is always `rank-one`.
    - Since $U$ and $V$ are both the vector product of a vector with itself, they are also symmetric.
    - So,
$$\begin{aligned}
B_{k+1} = B_{k} + a\vec{u}\vec{u}^T + b\vec{v}\vec{v}^T
\end{aligned}$$
3. Since $U_k$ and $V_k$ are `rank-one` (and $\vec{u}$ and $\vec{v}$ are linearly independent). `Their sum is rank-two`, and an update of this form is known as as a `rank-two update`.
4. Thus if we `start out with a symmetric matrix` $B_0$, the $B_k$ will `remain symmetric` for all $k$. The `rank-two` condition guarantees the `closeness` of $B_k$ and $B_{k+1}$ at each iteration.
5. Furthermore, we have to impose the `quasi-Newton condition`:
$$\begin{aligned}
B_{k+1}\Delta x_k = B_k \Delta x_k + a\vec{u}\vec{u}^T \Delta x_k + b\vec{v}\vec{v}^T \Delta x_k = y_k
\end{aligned}$$
6. At this point, a natural choice for $\vec{u}$ and $\vec{v}$ would be 
$$\begin{cases}
\vec{u} = y_k \\
\vec{v} = B_k \Delta x_k
\end{cases}$$
7. 将 `6` 带入到 `5` 中
$$\begin{aligned}
B_k \Delta x_k + ay_ky_k^T\Delta x_k + bB_k\Delta x_k \Delta x_k^T B_k^T \Delta x_k = y_k   \\ 
y_k (1 - ay_k^T\Delta x_k) = B_k \Delta x_k ( 1 + b\Delta x_k^TB_k^T\Delta x_k )
\end{aligned}$$
8. So, 
$$\begin{aligned}
a &= \frac{1}{y_k^T \Delta x_k}  \\
b &= -\frac{1}{x_k^T B_k \Delta x_k}
\end{aligned}$$
9. <font color="red">We have the `BFGS update` in all its glory:
    - Note: $B$ is `approximation for Hessian matrix`</font>
$$\begin{aligned}
B_{k+1} = B_{k} + \frac{y_k y_k^T}{y_k^T\Delta x_k} - \frac{B_k\Delta x_k \Delta x_k^T B_k}{\Delta x_k^T B_k \Delta x_k}
\end{aligned}$$

</font>


## 3.3. Deritives of `BFGS` -- Update `inverse approximation of Hessian Matrix` $B_k^{-1}$
<font color="steelblue" size="4">

1. So far, we are updating the `approximate Hessian matrix` at each iteration `using only previous gradient information`.
2. Note, however, that in practice we actually want to compute $B^{-1}$ directly, besause it's the `inverse Hessian matrix that appears in a Newton update`.
3. To invert formula in `3.3.2.9.`. We make use of the `Woodbury formula`, which tells us how to invert the sum of an invrtible matrix A and a rank-k correction:
$$\begin{aligned}
(A + UCV)^{-1} = A^{-1} - A^{-1}U[C^{-1}+VA^{-1}U]^{-1}VA^{-1}
\end{aligned}$$
4. We first rewrite the update in a more suitable form for applying the `Woodbury formula`
    - To avoid clutter, we will supress the subscripts $k+1$ and $k$, instead denoting the update as $B_{+}$
$$\begin{aligned}
(A + UCV)^{-1}
&= A^{-1} - A^{-1}U[C^{-1}+VA^{-1}U]^{-1}VA^{-1}  \\
&= B + \begin{bmatrix}B\Delta x & y\end{bmatrix} \begin{bmatrix}-\frac{1}{\Delta x^T B \Delta x} & 0 \\ 0 & \frac{1}{y^T\Delta x}\end{bmatrix} \begin{bmatrix}\Delta x^TB \\ y^T\end{bmatrix} \\
&= B + UCV
\end{aligned}$$
5. Plugging formula in `4` into `Woodbury formula`,
$$\begin{aligned}
B^{-1}_{+} 
&= B^{-1} - B^{-1}\begin{bmatrix}B \Delta x & y\end{bmatrix} \left[ \begin{bmatrix}-\Delta x^TB\Delta x & 0 \\ 0 & y^T  \end{bmatrix} + \begin{bmatrix}\Delta x^TB \\ y^T\end{bmatrix}B^{-1} \begin{bmatrix}B\Delta x & y\end{bmatrix} \right]^{-1} \begin{bmatrix}\Delta x^TB \\ y^T\end{bmatrix}B^{-1} \\
&= B^{-1} - \begin{bmatrix}\Delta x & B^{-1}y\end{bmatrix}
\begin{bmatrix}
-\frac{y^T\Delta x + y^TB^{-1}y}{y^T\Delta x \Delta x^T y} & \frac{\Delta x^T y}{y^T \Delta x \Delta x^T y} \\
\frac{y^T\Delta x}{y^T \Delta x \Delta x^T y} & 0
\end{bmatrix}
\begin{bmatrix}
\Delta x^T \\
y^T B^{-1}
\end{bmatrix} \\
&=B^{-1} + \frac{(\Delta x - B^{-1}y)\Delta x^T}{y^T \Delta x} + \frac{\Delta x(\Delta x - B^{-1}y)^T}{y^T \Delta x} - \frac{(\Delta x - B^{-1}y)^T}{(y^T\Delta x)^2}\Delta x \Delta x^T
\end{aligned}$$ 
6. <font color="red">In summary, the `approximation of Hessian matrix` $B_{+}^{-1}$:</font>
$$\begin{aligned}
B^{-1}_+ = \left( I - \frac{\Delta x y^T}{y^T\Delta x} \right)B^{-1} \left( I - \frac{y\Delta x^T}{y^T\Delta x} \right) + \frac{\Delta x \Delta x^T}{y^T \Delta x}
\end{aligned}$$

</font>


<font color="coral" size="4">

Note
----
1. We are now also in a position to show that our `rank-two` preserves `positive-definiteness`:
$$\begin{aligned}
z^TB_{k+1}^{-1}z = \left( z - \frac{\Delta x^Tz}{y^T\Delta x}y \right)^T B_k^{-1} \left( z - \frac{\Delta x^Tz}{y^T\Delta x}y \right) + \frac{\left( \Delta x^T z \right)^2}{y^T \Delta x}
\end{aligned}$$
2. For a non-zero vector $\vec{z}$. If $B_k^{-1}$ is `positive-definiteness`, then both terms are non-negative with the second term being zero only if $\Delta x^T z = 0$. `Positive-definiteness` is thus preserved by `BFGS's rank-two update`. 
3. If we use `rank-one` update instead of $B_{k+1} = B_k + a\vec{u}\vec{u}^T + b\vec{v}\vec{v}^T$, we wouldn't end up with a `positive-definite` preserving update.
4. This is why `BFGS` uses a `rank-two update`: It is the update with `the lowest rank` that preserves `positive-definiteness`.

</font>

## 3.4. `BFGS` 的特点
<font color="steelblue" size="4">

1. Each update is made `only requring previous gradient information`.
2. At each iteration, we update the value of $B^{-1}$ using only the values of `previous step` $k-1$ and `current step` $k$
$$\begin{cases}
\Delta x_k = x_{k} - x_{k-1} \\
y_k = \nabla f(x_{k}) - \nabla f(x_{k-1})
\end{cases}$$

</font>


## 3.5. How to kick off the algorithm?
<font color="steelblue" size="4">

1. One final implementation detail that we previously glossed over:
    - <font color="#73db90">Since an update of $B^{-1}$ depends on its previous value, we have to `initialize` $B_0^{-1}$ to `kick off the algorihtm`</font>
2. `First way`: Set $B_0^{-1}$ to the `identity matrix`, in which case the first step will be equivalent to vanilla gradient. And subsequent updates will incrementally refine it to get easier to the `inverse Hessian matrix`.
3. `Second way`: `Compute and invert the true Hessian matrix at initial point`. This would start the algorithm off with mote efficient steps, but comes at an initial cost of computing the true Hessian matrix and invert it.

</font>