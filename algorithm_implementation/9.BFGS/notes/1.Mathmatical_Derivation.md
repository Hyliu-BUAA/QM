<font color="#73BD90" size="4">

Website
-------
1. https://towardsdatascience.com/bfgs-in-a-nutshell-an-introduction-to-quasi-newton-methods-21b0e13ee504
    1. `Gradient descent` reviewed
    2. `Newton's Method`
    3. `Quasi-Newton Method`
    4. `BFGS optimization`

</font>


# 1. Newton Method
<font color="steelblue" size="4">

1. The `aim` of `Newton Method`:
$$\begin{aligned}
\min_{\vec{x}\in\mathbb{R^n}}{f(\vec{x})}
\end{aligned}$$
2. 将 $f(x)$ 做二阶展开
$$\begin{cases}
f(x+\mathbb{v}) = f(x) + \nabla f(x)^T\mathbb{v} \\
\mathbb{v} = \Delta x
\end{cases}$$
3. 我们需要 $f(x+\mathbb{v})$ 最小，我们把 $x$ 看做一个常数，使用 $f(x+\mathbb{v})$ 对 $\mathbb{v}$ 求导：
$$\begin{aligned}
\frac{\partial}{\partial \mathbb{v}}f(x+\mathbb{v}) = \nabla f(x) + \nabla^2f(x)\mathbb{v}=0
\end{aligned}$$
4. <font color="red">`Search direction` of `Newton Method`:</font>
$$\begin{aligned}
\mathbb{v} = -\nabla^2f(x)^{-1}\nabla f(x)
\end{aligned}$$
5. `Hessian Matrix`:
$$\begin{aligned}
\nabla^2f(x)
\end{aligned}$$
6. `Hessian Matrix` 的`正定性`：
$$\begin{aligned}
\nabla{f(x)^T} \mathbb{v} = -\nabla f(x)^T \nabla^2 f(x)^{-1}\nabla{f(x)} \ge 0
\end{aligned}$$

</font>